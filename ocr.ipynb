{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction done! Check the directory: ./data\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# Define the path to the .tgz file and the target directory for extraction\n",
    "tgz_path = './data/EnglishFnt.tgz'\n",
    "target_path = './data'\n",
    "\n",
    "# Uncompress the .tgz file\n",
    "with tarfile.open(tgz_path, 'r:gz') as tgz_ref:\n",
    "    tgz_ref.extractall(target_path)\n",
    "\n",
    "print(f'Extraction done! Check the directory: {target_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Fnt directory at: ./data\\English\\Fnt\n",
      "Contents: ['Sample001', 'Sample002', 'Sample003', 'Sample004', 'Sample005', 'Sample006', 'Sample007', 'Sample008', 'Sample009', 'Sample010', 'Sample011', 'Sample012', 'Sample013', 'Sample014', 'Sample015', 'Sample016', 'Sample017', 'Sample018', 'Sample019', 'Sample020', 'Sample021', 'Sample022', 'Sample023', 'Sample024', 'Sample025', 'Sample026', 'Sample027', 'Sample028', 'Sample029', 'Sample030', 'Sample031', 'Sample032', 'Sample033', 'Sample034', 'Sample035', 'Sample036', 'Sample037', 'Sample038', 'Sample039', 'Sample040', 'Sample041', 'Sample042', 'Sample043', 'Sample044', 'Sample045', 'Sample046', 'Sample047', 'Sample048', 'Sample049', 'Sample050', 'Sample051', 'Sample052', 'Sample053', 'Sample054', 'Sample055', 'Sample056', 'Sample057', 'Sample058', 'Sample059', 'Sample060', 'Sample061', 'Sample062']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "fnt_dir = os.path.join(target_path, 'English', 'Fnt')\n",
    "if os.path.exists(fnt_dir):\n",
    "    print(f'Found Fnt directory at: {fnt_dir}')\n",
    "    print('Contents:', os.listdir(fnt_dir))\n",
    "else:\n",
    "    print(f'Directory not found: {fnt_dir}')\n",
    "\n",
    "\n",
    "train_dir = './data/English/Fnt_train'\n",
    "val_dir = './data/English/Fnt_val'\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "val_split = 0.2  # Split ratio for validation\n",
    "\n",
    "for folder_name in os.listdir(fnt_dir):\n",
    "    character_folder_path = os.path.join(fnt_dir, folder_name)\n",
    "    \n",
    "    if os.path.isdir(character_folder_path):\n",
    "        images = os.listdir(character_folder_path)\n",
    "        \n",
    "        if images:\n",
    "            train_images, val_images = train_test_split(images, test_size=val_split)\n",
    "            \n",
    "            train_character_folder = os.path.join(train_dir, folder_name)\n",
    "            val_character_folder = os.path.join(val_dir, folder_name)\n",
    "            \n",
    "            os.makedirs(train_character_folder, exist_ok=True)\n",
    "            os.makedirs(val_character_folder, exist_ok=True)\n",
    "            \n",
    "            for image in train_images:\n",
    "                src_path = os.path.join(character_folder_path, image)\n",
    "                dst_path = os.path.join(train_character_folder, image)\n",
    "                if not os.path.exists(dst_path):  # Check if the file already exists\n",
    "                    shutil.move(src_path, dst_path)\n",
    "                else:\n",
    "                    print(f\"File already exists: {dst_path}\")\n",
    "                \n",
    "            for image in val_images:\n",
    "                src_path = os.path.join(character_folder_path, image)\n",
    "                dst_path = os.path.join(val_character_folder, image)\n",
    "                if not os.path.exists(dst_path):  # Check if the file already exists\n",
    "                    shutil.move(src_path, dst_path)\n",
    "                else:\n",
    "                    print(f\"File already exists: {dst_path}\")\n",
    "        else:\n",
    "            print(f\"No images found in folder {folder_name}. Skipping...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images in Sample001: 812\n",
      "Validation images in Sample001: 204\n",
      "Training images in Sample002: 812\n",
      "Validation images in Sample002: 204\n",
      "Training images in Sample003: 812\n",
      "Validation images in Sample003: 204\n",
      "Training images in Sample004: 812\n",
      "Validation images in Sample004: 204\n",
      "Training images in Sample005: 812\n",
      "Validation images in Sample005: 204\n"
     ]
    }
   ],
   "source": [
    "# Check a few folders to ensure they have been populated correctly\n",
    "for folder_name in os.listdir(train_dir)[:5]:  # Check the first 5 folders\n",
    "    train_character_folder = os.path.join(train_dir, folder_name)\n",
    "    val_character_folder = os.path.join(val_dir, folder_name)\n",
    "    \n",
    "    print(f\"Training images in {folder_name}: {len(os.listdir(train_character_folder))}\")\n",
    "    print(f\"Validation images in {folder_name}: {len(os.listdir(val_character_folder))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50344 images belonging to 62 classes.\n",
      "Found 12648 images belonging to 62 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define your data generator for training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255  # Normalize the image\n",
    ")\n",
    "\n",
    "# Define your data generator for validation data\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255  # Normalize the image\n",
    ")\n",
    "\n",
    "# Load images from the training directory\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),  # Resize images to 224x224\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'  # For multi-class classification\n",
    ")\n",
    "\n",
    "# Load images from the validation directory\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),  # Resize images to 224x224\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'  # For multi-class classification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 50344\n",
      "Number of validation samples: 12648\n"
     ]
    }
   ],
   "source": [
    "print('Number of training samples:', train_generator.samples)\n",
    "print('Number of validation samples:', validation_generator.samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 1s 0us/step\n",
      "Number of training samples: 50344\n",
      "Number of validation samples: 12648\n",
      "Steps per epoch: 1573\n",
      "Validation steps: 395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m1000\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Load MobileNetV2\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False)\n",
    "\n",
    "# Freeze the base_model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "predictions = Dense(62, activation='softmax')(x)  # Use 62 for 62 classes\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "steps_per_epoch = max(1, train_generator.samples // train_generator.batch_size)\n",
    "validation_steps = max(1, validation_generator.samples // validation_generator.batch_size)\n",
    "\n",
    "print('Number of training samples:', train_generator.samples)\n",
    "print('Number of validation samples:', validation_generator.samples)\n",
    "print('Steps per epoch:', steps_per_epoch)\n",
    "print('Validation steps:', validation_steps)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1573/1573 [==============================] - 264s 168ms/step - loss: 0.3248 - accuracy: 0.8856 - val_loss: 0.3747 - val_accuracy: 0.8719\n",
      "Epoch 2/20\n",
      "1573/1573 [==============================] - 55s 35ms/step - loss: 0.3084 - accuracy: 0.8920 - val_loss: 0.3687 - val_accuracy: 0.8725\n",
      "Epoch 3/20\n",
      "1573/1573 [==============================] - 54s 35ms/step - loss: 0.2925 - accuracy: 0.8960 - val_loss: 0.3578 - val_accuracy: 0.8755\n",
      "Epoch 4/20\n",
      "1573/1573 [==============================] - 57s 36ms/step - loss: 0.2783 - accuracy: 0.8992 - val_loss: 0.3508 - val_accuracy: 0.8785\n",
      "Epoch 5/20\n",
      "1573/1573 [==============================] - 53s 34ms/step - loss: 0.2671 - accuracy: 0.9017 - val_loss: 0.3389 - val_accuracy: 0.8826\n",
      "Epoch 6/20\n",
      "1573/1573 [==============================] - 50s 32ms/step - loss: 0.2569 - accuracy: 0.9067 - val_loss: 0.3365 - val_accuracy: 0.8801\n",
      "Epoch 7/20\n",
      "1573/1573 [==============================] - 54s 34ms/step - loss: 0.2458 - accuracy: 0.9097 - val_loss: 0.3295 - val_accuracy: 0.8802\n",
      "Epoch 8/20\n",
      "1573/1573 [==============================] - 52s 33ms/step - loss: 0.2373 - accuracy: 0.9133 - val_loss: 0.3263 - val_accuracy: 0.8841\n",
      "Epoch 9/20\n",
      "1573/1573 [==============================] - 51s 32ms/step - loss: 0.2282 - accuracy: 0.9159 - val_loss: 0.3206 - val_accuracy: 0.8830\n",
      "Epoch 10/20\n",
      "1573/1573 [==============================] - 52s 33ms/step - loss: 0.2208 - accuracy: 0.9177 - val_loss: 0.3181 - val_accuracy: 0.8840\n",
      "Epoch 11/20\n",
      "1573/1573 [==============================] - 57s 36ms/step - loss: 0.2151 - accuracy: 0.9198 - val_loss: 0.3060 - val_accuracy: 0.8868\n",
      "Epoch 12/20\n",
      "1573/1573 [==============================] - 54s 35ms/step - loss: 0.2072 - accuracy: 0.9220 - val_loss: 0.3116 - val_accuracy: 0.8855\n",
      "Epoch 13/20\n",
      "1573/1573 [==============================] - 54s 34ms/step - loss: 0.2036 - accuracy: 0.9238 - val_loss: 0.3068 - val_accuracy: 0.8879\n",
      "Epoch 14/20\n",
      "1573/1573 [==============================] - 62s 39ms/step - loss: 0.1983 - accuracy: 0.9254 - val_loss: 0.3062 - val_accuracy: 0.8877\n",
      "Epoch 15/20\n",
      "1573/1573 [==============================] - 61s 39ms/step - loss: 0.1919 - accuracy: 0.9267 - val_loss: 0.3008 - val_accuracy: 0.8880\n",
      "Epoch 16/20\n",
      "1573/1573 [==============================] - 58s 37ms/step - loss: 0.1869 - accuracy: 0.9280 - val_loss: 0.3005 - val_accuracy: 0.8910\n",
      "Epoch 17/20\n",
      "1573/1573 [==============================] - 56s 36ms/step - loss: 0.1807 - accuracy: 0.9311 - val_loss: 0.2945 - val_accuracy: 0.8896\n",
      "Epoch 18/20\n",
      "1573/1573 [==============================] - 55s 35ms/step - loss: 0.1785 - accuracy: 0.9324 - val_loss: 0.2963 - val_accuracy: 0.8903\n",
      "Epoch 19/20\n",
      "1573/1573 [==============================] - 56s 36ms/step - loss: 0.1739 - accuracy: 0.9332 - val_loss: 0.2950 - val_accuracy: 0.8934\n",
      "Epoch 20/20\n",
      "1573/1573 [==============================] - 51s 33ms/step - loss: 0.1719 - accuracy: 0.9333 - val_loss: 0.2893 - val_accuracy: 0.8949\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/ocr\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/ocr\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./models/ocr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def non_max_suppression(boxes, probs=None, overlapThresh=0.5):\n",
    "    # If there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # If the bounding boxes are integers, convert them to floats -- this\n",
    "    # is important since we'll be doing a bunch of divisions\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "\n",
    "    # Initialize the list of picked indexes\n",
    "    pick = []\n",
    "\n",
    "    # Grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    # Compute the area of the bounding boxes and sort the bounding\n",
    "    # boxes by the bottom-right y-coordinate of the bounding box\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "\n",
    "    # Keep looping while some indexes still remain in the indexes list\n",
    "    while len(idxs) > 0:\n",
    "        # Grab the last index in the indexes list, add the index\n",
    "        # value to the list of picked indexes, then initialize\n",
    "        # the suppression list (i.e., indexes that will be deleted)\n",
    "        # using the last index\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "        suppress = [last]\n",
    "\n",
    "        # Loop over all indexes in the indexes list\n",
    "        for pos in range(0, last):\n",
    "            # Grab the current index\n",
    "            j = idxs[pos]\n",
    "\n",
    "            # Find the largest (x, y) coordinates for the start of\n",
    "            # the bounding box and the smallest (x, y) coordinates\n",
    "            # for the end of the bounding box\n",
    "            xx1 = max(x1[i], x1[j])\n",
    "            yy1 = max(y1[i], y1[j])\n",
    "            xx2 = min(x2[i], x2[j])\n",
    "            yy2 = min(y2[i], y2[j])\n",
    "\n",
    "            # Compute the width and height of the bounding box\n",
    "            w = max(0, xx2 - xx1 + 1)\n",
    "            h = max(0, yy2 - yy1 + 1)\n",
    "\n",
    "            # Compute the ratio of overlap between the computed\n",
    "            # bounding box and the bounding box in the area list\n",
    "            overlap = float(w * h) / area[j]\n",
    "\n",
    "            # If there is sufficient overlap, suppress the\n",
    "            # current bounding box\n",
    "            if overlap > overlapThresh:\n",
    "                suppress.append(pos)\n",
    "\n",
    "        # Delete all indexes from the index list that are in the\n",
    "        # suppression list\n",
    "        idxs = np.delete(idxs, suppress)\n",
    "\n",
    "    # Return only the bounding boxes that were picked\n",
    "    return boxes[pick].astype(\"int\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def decode_predictions(scores, geometry, min_confidence):\n",
    "    # Grab the number of rows and columns from the scores volume, then initialize\n",
    "    # our set of bounding box rectangles and corresponding confidence scores\n",
    "    (numRows, numCols) = scores.shape[2:4]\n",
    "    rects = []\n",
    "    confidences = []\n",
    "\n",
    "    # Loop over the number of rows\n",
    "    for y in range(0, numRows):\n",
    "        # Extract the scores (probabilities), followed by the geometrical\n",
    "        # data used to derive potential bounding box coordinates that\n",
    "        # surround text\n",
    "        scoresData = scores[0, 0, y]\n",
    "        xData0 = geometry[0, 0, y]\n",
    "        xData1 = geometry[0, 1, y]\n",
    "        xData2 = geometry[0, 2, y]\n",
    "        xData3 = geometry[0, 3, y]\n",
    "        anglesData = geometry[0, 4, y]\n",
    "\n",
    "        # Loop over the number of columns\n",
    "        for x in range(0, numCols):\n",
    "            # If our score does not have sufficient probability, ignore it\n",
    "            if scoresData[x] < min_confidence:\n",
    "                continue\n",
    "\n",
    "            # Compute the offset factor as our resulting feature maps will\n",
    "            # be 4x smaller than the input image\n",
    "            (offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
    "\n",
    "            # Extract the rotation angle for the prediction and then\n",
    "            # compute the sin and cosine\n",
    "            angle = anglesData[x]\n",
    "            cos = np.cos(angle)\n",
    "            sin = np.sin(angle)\n",
    "\n",
    "            # Use the geometry volume to derive the width and height of\n",
    "            # the bounding box\n",
    "            h = xData0[x] + xData2[x]\n",
    "            w = xData1[x] + xData3[x]\n",
    "\n",
    "            # Compute both the starting and ending (x, y)-coordinates for\n",
    "            # the text prediction bounding box\n",
    "            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
    "            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
    "            startX = int(endX - w)\n",
    "            startY = int(endY - h)\n",
    "\n",
    "            # Add the bounding box coordinates and probability score to\n",
    "            # our respective lists\n",
    "            rects.append((startX, startY, endX, endY))\n",
    "            confidences.append(scoresData[x])\n",
    "\n",
    "    # Return a tuple of the bounding boxes and associated confidences\n",
    "    return (rects, confidences)\n",
    "\n",
    "# Load the pre-trained EAST text detector\n",
    "net = cv2.dnn.readNet('./models/frozen_east_text_detection.pb')\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread('test_image.jpeg')\n",
    "orig = image.copy()\n",
    "(origH, origW) = image.shape[:2]\n",
    "\n",
    "# Set the new width and height and determine the ratio in change\n",
    "(newW, newH) = (320, 320)\n",
    "rW = origW / float(newW)\n",
    "rH = origH / float(newH)\n",
    "\n",
    "# Resize the image and grab the new image dimensions\n",
    "image = cv2.resize(image, (newW, newH))\n",
    "(H, W) = image.shape[:2]\n",
    "\n",
    "# Define the two output layer names for the EAST detector model\n",
    "layerNames = [\n",
    "    \"feature_fusion/Conv_7/Sigmoid\",\n",
    "    \"feature_fusion/concat_3\"\n",
    "]\n",
    "\n",
    "# Forward pass of the blob through the network\n",
    "blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n",
    "                             (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "(scores, geometry) = net.forward(layerNames)\n",
    "\n",
    "# Decode the predictions\n",
    "(rects, confidences) = decode_predictions(scores, geometry, min_confidence=0.5)\n",
    "\n",
    "# Apply non-maxima suppression to suppress weak, overlapping bounding boxes\n",
    "boxes = non_max_suppression(np.array(rects), probs=confidences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "\n",
    "def preprocess_image(image, target_size):\n",
    "    # Convert the image from BGR (OpenCV format) to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert the image from NumPy array to PIL Image\n",
    "    image = Image.fromarray(image)\n",
    "\n",
    "    # Check the mode and resize\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    image = image.resize(target_size)\n",
    "\n",
    "    # Convert the image to array and preprocess it\n",
    "    image = preprocess_input(np.array(image))\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: 'A', 11: 'B', 12: 'C', 13: 'D', 14: 'E', 15: 'F', 16: 'G', 17: 'H', 18: 'I', 19: 'J', 20: 'K', 21: 'L', 22: 'M', 23: 'N', 24: 'O', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'T', 30: 'U', 31: 'V', 32: 'W', 33: 'X', 34: 'Y', 35: 'Z', 36: 'a', 37: 'b', 38: 'c', 39: 'd', 40: 'e', 41: 'f', 42: 'g', 43: 'h', 44: 'i', 45: 'j', 46: 'k', 47: 'l', 48: 'm', 49: 'n', 50: 'o', 51: 'p', 52: 'q', 53: 'r', 54: 's', 55: 't', 56: 'u', 57: 'v', 58: 'w', 59: 'x', 60: 'y', 61: 'z'}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Digits\n",
    "digits = list(string.digits)\n",
    "\n",
    "# Uppercase characters\n",
    "uppercase_chars = list(string.ascii_uppercase)\n",
    "\n",
    "# Lowercase characters\n",
    "lowercase_chars = list(string.ascii_lowercase)\n",
    "\n",
    "# Combine all the characters to form the class labels, starting with digits\n",
    "all_chars = digits + uppercase_chars + lowercase_chars\n",
    "\n",
    "# Create the index_to_char dictionary\n",
    "index_to_char = {i: all_chars[i] for i in range(len(all_chars))}\n",
    "\n",
    "print(index_to_char)  # Display the mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_characters(roi, draw_rectangles=True):\n",
    "    # Convert ROI to grayscale and apply threshold\n",
    "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    # Use cv2.THRESH_BINARY instead of cv2.THRESH_BINARY_INV\n",
    "    thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # Find contours and sort them from left-to-right\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    bounding_boxes = [cv2.boundingRect(c) for c in contours]\n",
    "    (contours, bounding_boxes) = zip(*sorted(zip(contours, bounding_boxes),\n",
    "                                            key=lambda b: b[1][0],\n",
    "                                            reverse=False))\n",
    "    character_images = []\n",
    "    char_coords = []  # List to store the coordinates of each character\n",
    "\n",
    "    for contour in contours:\n",
    "        # Get rectangle bounding contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Draw rectangles (optional)\n",
    "        if draw_rectangles:\n",
    "            cv2.rectangle(roi, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Store the coordinates of the character\n",
    "        char_coords.append((x, y, w, h))\n",
    "\n",
    "        # Getting ROI\n",
    "        char_roi = roi[y:y + h, x:x + w]\n",
    "\n",
    "        # Preprocess the ROI for character recognition model\n",
    "        # Make sure the preprocess_image function is defined and compatible with your model\n",
    "        char_roi_processed = preprocess_image(char_roi, target_size=(224, 224))\n",
    "        character_images.append(char_roi_processed)\n",
    "\n",
    "    return roi, character_images, char_coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for (startX, startY, endX, endY) in boxes:\n",
    "    # Scale the bounding box coordinates based on the respective ratios\n",
    "    startX = int(startX * rW)\n",
    "    startY = int(startY * rH)\n",
    "    endX = int(endX * rW)\n",
    "    endY = int(endY * rH)\n",
    "\n",
    "    # Extract the region of interest\n",
    "    roi = orig[startY:endY, startX:endX]\n",
    "\n",
    "    # Segment characters in the ROI and get their coordinates\n",
    "    _, character_images, char_coords = segment_characters(roi)\n",
    "\n",
    "    # Draw bounding box for each character on the original image\n",
    "    for (x, y, w, h) in char_coords:\n",
    "        cv2.rectangle(orig, (startX + x, startY + y), (startX + x + w, startY + y + h), (0, 0, 255), 2)\n",
    "\n",
    "# Show the output image with bounding boxes around each character\n",
    "cv2.imshow(\"Image\",orig)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "48\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "40\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "33\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "48\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "\n",
    "detected_text = \"\"  # To store detected text from all boxes\n",
    "\n",
    "for (startX, startY, endX, endY) in boxes:\n",
    "    # Scale the bounding box coordinates based on the respective ratios\n",
    "    startX = int(startX * rW)\n",
    "    startY = int(startY * rH)\n",
    "    endX = int(endX * rW)\n",
    "    endY = int(endY * rH)\n",
    "\n",
    "    # Extract the region of interest\n",
    "    roi = orig[startY:endY, startX:endX]\n",
    "\n",
    "    # Segment characters in the ROI and get their coordinates\n",
    "    roi_with_characters, character_images, char_coords = segment_characters(roi)\n",
    "\n",
    "     # Recognize each character and draw bounding box around it\n",
    "    for char_img, (x, y, w, h) in zip(character_images, char_coords):\n",
    "        prediction = model.predict(char_img)\n",
    "        predicted_class = np.argmax(prediction, axis=1)\n",
    "        predicted_char = index_to_char[predicted_class[0]]  # Translate class index to character\n",
    "        print(predicted_class[0])\n",
    "\n",
    "        # Append the predicted character to the detected_text string\n",
    "        detected_text += predicted_char\n",
    "\n",
    "        # Draw bounding box for each character on the original image\n",
    "        cv2.rectangle(orig, (startX + x, startY + y), (startX + x + w, startY + y + h), (0, 0, 255), 2)\n",
    "        cv2.putText(orig, predicted_char, (startX + x, startY + y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    # Reset detected_text for the next box\n",
    "    detected_text = \"\"\n",
    "\n",
    "# Show the output image with bounding boxes around each character\n",
    "cv2.imshow(\"Image\",orig)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
